## 리뷰
## 주요 진행 활동
- 미니 프로젝트 회의
- 미션 2 진행
- 과거 주차 미션 결과물 재검토 후, 보완할 점 수정.

## 새롭게 알게 된 내용

    spark-submit --master spark://spark-master:7077 /tmp/pi.py 10

    spark-submit /tmp/pi.py 10

이 둘의 차이점은 Deploy option에 차이가 생긴다. 첫번째 명령어는 docker-compose.yml 파일에서 지정한 spark-master의 포트 번호로 지정하여 standalone cluster에서 실행하게 한다. 그렇기에 8080 Web Ui에서 수행한 pi.py가 completed application에 결과가 보이게 된다.</br>
하지만 두번째 명령어는 --master를 지정하지 않았기에 자동적으로 local mode로 실행하게 되어 completed application에 결과가 보여지지 않는다.

pyspark의 데이터프레임은 correlation_results_spark.write.csv를 통해서 csv 저장이 가능하다.

데이터 프레임 출력, 시각화와 상관 관계 분석을 할 때는 pyspark dataframe 형식을 pandas로 변환시켜야만 한다.

## 느낀점
pyspark.sql을 가지고 미션 2를 진행을 하였는데, 명령어는 비슷하더라도, 실질적인 문법과 명령어의 대소문자 차이 등 차이가 존재하여 AI를 이용하지 않고, 혼자 코드를 작성하기는 힘든 부분이 있었다. 그래도, 열정적으로 학습에 임한 덕분에 나름 pyspark에 익숙해진 것 같고, 이를 최종 프로젝트에서도 잘 사용할 수 있도록 더욱 연마해야겠다.

## 앞으로의 학습 방향
내일 출근하면 팀원들과 같이 spark에 대해서 이야기하며 더 깊게 이해하는 시간을 가져야겠다. 혼자 공부하는 것도 좋지만, 팀원들과 소통하고 같이 배워나가는 것이 훨씬 더 이해가 잘되고 효율적인 것 같다.</br>
미니 프로젝트도 지금까지 진행한 것을 함께 정리하고 완성된 프로덕트가 나올 수 있도록 내일 잘 마무리하고 싶다.</br>
추가적으로 pyspark를 현재 로컬에서 사용하였는데, 다른 학생들 중 몇몇은 이를 미션 1에 만든 spark 컨테이너에 연결하여 사용하였다. 이것 관련해서 내일 물어보고, 시도해보아야겠다.

## 회고
## Keep
오늘도 10시부터 공부하러 카페에 나가 집중을 유지하며 저녁 7시까지 학습을 진행하였다. 연휴의 마무리 날까지 정규 학습 시간을 잘 지켰다고 생각한다. 2월이 곧 시작되면, 더욱 더 공부할 양도 많아지고, 최종 프로젝트도 진행해야할텐데, 이 경험을 바탕으로 쉬는 요일이나 정규 시간 이외 시간에 부지런히 내 할 일을 할 수 있을 것이라는 자신감이 생겼다.

## Problem
오늘도 Github pull-request 때문에 상당히 오랜 시간을 소비했다. 그래도 여태까지 공부한 것 덕분에 상황에 맞게 대처는 했지만, 경험 부족이 문제인지 예상치 못한 오류가 발생하면 혹시나 문서가 날아갈까봐 스트레스를 항상 많이 받는 것 같다. 이번 경험을 통해서 또, 이런 식으로 시간을 낭비하지 않도록 해야겠다.

## Try