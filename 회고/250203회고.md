## 리뷰
## 주요 진행 활동
- 다노님 RDD 강의
- 5주차 미션 진행
- 현업자 멘토링 질문 작성
- 프로젝트 관련 미팅

## 새롭게 알게 된 내용

sc.textFile(file_path)는 Spark RDD를 생성하는 방법 중 하나이다.(sc는 SparkContext를 의미.)

Transformation -> 새로운 RDD가 생성된다.
Action -> RDD가 나오지 않는다. 

각 Job에서 여러 순차적인 스테이지가 생성되고, 그 스테이지는 여러 작은 테스크들로 구성되어 있다.</br>
Task < Stage < Job</br>
Job이 끝나면 Result Stage가 만들어진다.

Boundary of Stages는 shuffle이 일어나는 지점이 된다. Shuffle 이후에 Wide transformation이 진행되기 때문에 Stage의 첫 transformation은 대부분 Wide 성격을 띤다.

Data Lakehouse는 Lake 위에 warehouse를 짓는 느낌이다. 구조적으로 데이터 레이크 위에 Medtadata and Governace Layer가 있는데 이곳에서 ETL 등이 일어나 분석 가능한 데이터를 만든다.

Data Warehouse : Schema-on-write, 데이터를 스키마 구조에 맞게 쓴다.</br>
Data Lake : Schema-on-read, 분석할 때 스키마를 정하면서 다양한 형식으로 데이터를 사용한다.

## 느낀점

## 앞으로의 학습 방향

## 회고
## Keep

## Problem

## Try