{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58110cc1-c52d-4aaa-bd8a-b4c3481ec3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.14\" 2025-01-21\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.14+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.14+0, mixed mode, sharing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.popen(\"java -version 2>&1\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad76de9-28da-4823-991e-d33c1d6e41c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b549e21-7bff-4c4b-9e21-2b9e61ac8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType\n",
    "import os\n",
    "\n",
    "# 1ï¸âƒ£ Spark ì„¸ì…˜ ìƒì„±\n",
    "spark = SparkSession.builder.appName(\"Yellow_Taxi_Anlaysis\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "# 2ï¸âƒ£ ëª¨ë“  .parquet íŒŒì¼ ê²½ë¡œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "directory_path = \"/Users/admin/Desktop/GitHub/softeer/ê³¼ì œ/M4/NYC_TLC_Trip_Data/\"\n",
    "file_paths = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith(\".parquet\")]\n",
    "\n",
    "# 3ï¸âƒ£ ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ í˜•ë³€í™˜ ì²˜ë¦¬ ë° ë³‘í•©\n",
    "df_list = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # 4ï¸âƒ£ ê° íŒŒì¼ ì½ê¸°\n",
    "    df = spark.read.parquet(file_path)\n",
    "    \n",
    "    # 5ï¸âƒ£ í˜•ë³€í™˜ (í•„ìš”í•œ ì»¬ëŸ¼ì— ëŒ€í•´)\n",
    "    df = df.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType())) \\\n",
    "           .withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType())) \\\n",
    "           .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType())) \\\n",
    "           .withColumn(\"passenger_count\", col(\"passenger_count\").cast(LongType())) \\\n",
    "           .withColumn(\"RatecodeID\", col(\"RatecodeID\").cast(LongType()))\n",
    "    \n",
    "    # 6ï¸âƒ£ ë³€í™˜ëœ DataFrame ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    df_list.append(df)\n",
    "\n",
    "# 7ï¸âƒ£ ë³‘í•©ëœ DataFrame ìƒì„±\n",
    "final_df = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    final_df = final_df.union(df)\n",
    "\n",
    "# 8ï¸âƒ£ ê²°ê³¼ ì¶œë ¥ (ìƒìœ„ 5ê°œ í–‰)\n",
    "final_df.show(5)\n",
    "\n",
    "# 9ï¸âƒ£ í•„ìš”ì‹œ ì €ì¥\n",
    "# final_df.write.parquet(\"/path/to/save/final_output.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce44d2-3ffc-433d-8c06-6aa785de05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# ê° ì»¬ëŸ¼ë³„ NULL ê°’ ê°œìˆ˜ í™•ì¸\n",
    "null_counts_df = final_df.select(\n",
    "    [spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in final_df.columns]\n",
    ")\n",
    "\n",
    "# Pandasë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥\n",
    "null_counts_pd = null_counts_df.toPandas()\n",
    "null_counts_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bdabe-32bc-4c35-ac9e-cde0d798f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a850f75-d6cd-4b0c-869d-aebf5ee42713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ï¸âƒ£ ì‹œê°„ ë°ì´í„° ë³€í™˜ (Timestamp ë³€í™˜ ë° trip_duration ì¶”ê°€)\n",
    "final_df = final_df.withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n",
    "                   .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f9303-999d-43a2-bf9f-b6277faf44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# PySpark DataFrame â†’ Pandas API on Spark DataFrame ë³€í™˜\n",
    "ps_df = final_df.pandas_api()\n",
    "\n",
    "# Pandasì²˜ëŸ¼ í‰ê·  ê³„ì‚° ê°€ëŠ¥\n",
    "avg_trip_duration = (ps_df[\"tpep_dropoff_datetime\"]-ps_df[\"tpep_pickup_datetime\"]).mean()\n",
    "avg_trip_distance = ps_df[\"trip_distance\"].mean()\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸ“Œ í‰ê·  ì´ë™ ì‹œê°„: {avg_trip_duration:.2f} ì´ˆ\")\n",
    "print(f\"ğŸ“Œ í‰ê·  ì´ë™ ê±°ë¦¬: {avg_trip_distance:.2f} ë§ˆì¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782e557-5f85-449b-ad28-56677d00356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe38ffb-b7f2-46e4-91b3-5509e5d85e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ace_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19396ac-0693-4fda-9413-a4ba9f84b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType, IntegerType, DoubleType, StringType\n",
    "\n",
    "# ìŠ¤íŒŒí¬ ì„¸ì…˜ ì‹œì‘\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ParquetFileReader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ë²¡í„°í™”ëœ ë¦¬ë” ë¹„í™œì„±í™”\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "# ì—¬ëŸ¬ íŒŒì¼ì„ ì½ê¸° ì „ì— ëª¨ë“  ì»¬ëŸ¼ì„ String íƒ€ì…ìœ¼ë¡œ ê°•ì œ ë³€í™˜\n",
    "df = spark.read.parquet(\"file:///Users/admin/Desktop/GitHub/softeer/ê³¼ì œ/M4/NYC_TLC_Trip_Data/*.parquet\")\n",
    "\n",
    "# ëª¨ë“  ì»¬ëŸ¼ì„ String íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
    "for col_name in df.columns:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(StringType()))\n",
    "\n",
    "# í•„ìš”í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
    "df = df.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType()))\n",
    "df = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(IntegerType()))\n",
    "df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(DoubleType()))\n",
    "df = df.withColumn(\"total_amount\", col(\"total_amount\").cast(DoubleType()))\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ ì¶œë ¥\n",
    "df.printSchema()\n",
    "\n",
    "# ë°ì´í„° í™•ì¸ (ìƒìœ„ 5ê°œ í–‰ ì¶œë ¥)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c784c9-8d1f-4def-8ba9-8647da104679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# 1ï¸âƒ£ Spark ì„¸ì…˜ ìƒì„±\n",
    "spark = SparkSession.builder.appName(\"YellowTaxiMerge\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "# 2ï¸âƒ£ ëª¨ë“  Parquet íŒŒì¼ì„ ì½ì–´ì„œ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³‘í•©\n",
    "file_path = \"/Users/admin/Desktop/GitHub/softeer/ê³¼ì œ/M4/NYC_TLC_Trip_Data/*.parquet\"\n",
    "df = spark.read.option(\"mergeSchema\", \"false\").parquet(file_path)\n",
    "\n",
    "# 3ï¸âƒ£ ìˆ˜ë™ìœ¼ë¡œ ì»¬ëŸ¼ íƒ€ì… ê°•ì œ ë³€í™˜ (LongTypeìœ¼ë¡œ ë³€í™˜)\n",
    "df = df.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType()))\n",
    "df = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(LongType()))\n",
    "df = df.withColumn(\"payment_type\", col(\"payment_type\").cast(LongType()))\n",
    "df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"extra\", col(\"extra\").cast(LongType()))\n",
    "df = df.withColumn(\"mta_tax\", col(\"mta_tax\").cast(LongType()))\n",
    "df = df.withColumn(\"tip_amount\", col(\"tip_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"improvement_surcharge\", col(\"improvement_surcharge\").cast(LongType()))\n",
    "df = df.withColumn(\"total_amount\", col(\"total_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"congestion_surcharge\", col(\"congestion_surcharge\").cast(LongType()))\n",
    "df = df.withColumn(\"airport_fee\", col(\"airport_fee\").cast(LongType()))\n",
    "\n",
    "# 4ï¸âƒ£ ìŠ¤í‚¤ë§ˆ ì¶œë ¥\n",
    "df.printSchema()\n",
    "\n",
    "# 5ï¸âƒ£ ë°ì´í„° í™•ì¸ (ìƒìœ„ 5ê°œ í–‰ ì¶œë ¥)\n",
    "df.take(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd8c08-aa75-4448-9eb0-07e577c9aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession ìƒì„±\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Schema Check\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "import glob\n",
    "\n",
    "parquet_files = glob.glob(\"/Users/admin/Desktop/GitHub/softeer/ê³¼ì œ/M4/NYC_TLC_Trip_Data/*.parquet\")\n",
    "\n",
    "# ê° íŒŒì¼ì˜ ìŠ¤í‚¤ë§ˆ í™•ì¸\n",
    "for file in parquet_files:\n",
    "    print(f\"\\nğŸ“‚ íŒŒì¼: {file}\")\n",
    "    df = spark.read.parquet(file)\n",
    "    df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80414da-9f2c-43f9-98d2-ab8fb0be22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# ê¸°ì¡´ SparkContext ì¢…ë£Œ\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "# ìƒˆë¡œìš´ Spark ì„¸ì…˜ ìƒì„±\n",
    "spark = SparkSession.builder.appName(\"YellowTaxiMerge\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e89866-7187-4894-a2c2-8d49828caf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1db9c-4c8b-4e52-ba97-97ab9ea5d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Spark ì„¸ì…˜ ìƒì„±\n",
    "spark = SparkSession.builder.appName(\"Yellow Taxi Data\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\").getOrCreate()\n",
    "\n",
    "# ì „ì²´ ë””ë ‰í„°ë¦¬ì—ì„œ Parquet íŒŒì¼ ì½ê¸°\n",
    "df = spark.read.parquet(\"NYC_TLC_Trip_Data/\") \\\n",
    "    .withColumn(\"VendorID\", col(\"VendorID\").cast(\"bigint\"))  # VendorIDë¥¼ bigintë¡œ ë³€í™˜\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ í™•ì¸ ë° ë°ì´í„° ì¶œë ¥\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe926c5-1b9d-42f1-b261-5bea3180190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44e787-de90-463c-9643-340b713c5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ SparkContextê°€ ìˆëŠ” ê²½ìš° ì¢…ë£Œ\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9a5e3-db0b-4637-bb97-6c5c9ae5c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ê¸°ì¡´ SparkSession ì¢…ë£Œ\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession to stop:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d41c3d-f09b-43b5-8cf7-7ca323735d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test Spark Session\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8906bf-cf54-4893-87bf-669d11bc3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ê¸°ì¡´ SparkSession ë° SparkContext ì¢…ë£Œ\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"No active SparkSession or failed to stop:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68590684-ed3f-4e3f-9534-42ea7f2bc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b594222-f425-41e5-9f10-f227d7527e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ê¸°ì¡´ SparkSession ì¢…ë£Œ\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped.\")\n",
    "except Exception as e:\n",
    "    print(\"No active SparkSession or failed to stop:\", e)\n",
    "\n",
    "# ìƒˆë¡œìš´ SparkSession ìƒì„±\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark ì¢…ë£Œ\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d85a2-7b44-4cfb-9183-4ea5d0ded82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ê¸°ì¡´ SparkSession ì¢…ë£Œ\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped.\")\n",
    "except Exception as e:\n",
    "    print(\"No active SparkSession or failed to stop:\", e)\n",
    "\n",
    "# ìƒˆë¡œìš´ SparkSession ìƒì„±\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark ì¢…ë£Œ\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e6127-e5b8-4a65-8c12-222b42d3d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession ìƒì„±\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark ì¢…ë£Œ\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ecaed-a1a0-4057-9b54-24ca0762f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"4041\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark ì¢…ë£Œ\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7e888-4c30-4194-b5ab-f3c66be46150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"4041\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.net.preferIPv4Stack=true\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.net.preferIPv4Stack=true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark ì¢…ë£Œ\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a516711-4905-4526-b1db-ca699c2cb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b868d1-c1cf-495f-b68f-ebd63298cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkTest\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163ff44-8704-41f3-a029-bcc63acb4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c957b4b-8a70-46f1-af3b-e9da2c69c179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (System)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
