{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58110cc1-c52d-4aaa-bd8a-b4c3481ec3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.14\" 2025-01-21\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.14+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.14+0, mixed mode, sharing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.popen(\"java -version 2>&1\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad76de9-28da-4823-991e-d33c1d6e41c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b549e21-7bff-4c4b-9e21-2b9e61ac8113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/30 00:25:54 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2023-06-01 00:08:48|  2023-06-01 00:29:41|              1|          3.4|         1|                 N|         140|         238|           1|       21.9|  3.5|    0.5|       6.7|         0.0|                  1.0|        33.6|                 2.5|        0.0|\n",
      "|       1| 2023-06-01 00:15:04|  2023-06-01 00:25:18|              0|          3.4|         1|                 N|          50|         151|           1|       15.6|  3.5|    0.5|       3.0|         0.0|                  1.0|        23.6|                 2.5|        0.0|\n",
      "|       1| 2023-06-01 00:48:24|  2023-06-01 01:07:07|              1|         10.2|         1|                 N|         138|          97|           1|       40.8| 7.75|    0.5|      10.0|         0.0|                  1.0|       60.05|                 0.0|       1.75|\n",
      "|       2| 2023-06-01 00:54:03|  2023-06-01 01:17:29|              3|         9.83|         1|                 N|         100|         244|           1|       39.4|  1.0|    0.5|      8.88|         0.0|                  1.0|       53.28|                 2.5|        0.0|\n",
      "|       2| 2023-06-01 00:18:44|  2023-06-01 00:27:18|              1|         1.17|         1|                 N|         137|         234|           1|        9.3|  1.0|    0.5|      0.72|         0.0|                  1.0|       15.02|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType\n",
    "import os\n",
    "\n",
    "# 1️⃣ Spark 세션 생성\n",
    "spark = SparkSession.builder.appName(\"Yellow_Taxi_Anlaysis\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "# 2️⃣ 모든 .parquet 파일 경로 자동으로 불러오기\n",
    "directory_path = \"/Users/admin/Desktop/GitHub/softeer/과제/M4/NYC_TLC_Trip_Data/\"\n",
    "file_paths = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith(\".parquet\")]\n",
    "\n",
    "# 3️⃣ 모든 파일에 대해 형변환 처리 및 병합\n",
    "df_list = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # 4️⃣ 각 파일 읽기\n",
    "    df = spark.read.parquet(file_path)\n",
    "    \n",
    "    # 5️⃣ 형변환 (필요한 컬럼에 대해)\n",
    "    df = df.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType())) \\\n",
    "           .withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType())) \\\n",
    "           .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType())) \\\n",
    "           .withColumn(\"passenger_count\", col(\"passenger_count\").cast(LongType())) \\\n",
    "           .withColumn(\"RatecodeID\", col(\"RatecodeID\").cast(LongType()))\n",
    "    \n",
    "    # 6️⃣ 변환된 DataFrame 리스트에 추가\n",
    "    df_list.append(df)\n",
    "\n",
    "# 7️⃣ 병합된 DataFrame 생성\n",
    "final_df = df_list[0]\n",
    "for df in df_list[1:]:\n",
    "    final_df = final_df.union(df)\n",
    "\n",
    "# 8️⃣ 결과 출력 (상위 5개 행)\n",
    "final_df.show(5)\n",
    "\n",
    "# 9️⃣ 필요시 저장\n",
    "# final_df.write.parquet(\"/path/to/save/final_output.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ce44d2-3ffc-433d-8c06-6aa785de05a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6048705</td>\n",
       "      <td>0</td>\n",
       "      <td>6048705</td>\n",
       "      <td>6048705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6048705</td>\n",
       "      <td>6048705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID  tpep_pickup_datetime  tpep_dropoff_datetime  passenger_count  \\\n",
       "0         0                     0                      0          6048705   \n",
       "\n",
       "   trip_distance  RatecodeID  store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0              0     6048705             6048705             0             0   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             0            0      0        0           0             0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
       "0                      0             0               6048705      6048705  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# 각 컬럼별 NULL 값 개수 확인\n",
    "null_counts_df = final_df.select(\n",
    "    [spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in final_df.columns]\n",
    ")\n",
    "\n",
    "# Pandas로 변환하여 출력\n",
    "null_counts_pd = null_counts_df.toPandas()\n",
    "null_counts_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9bdabe-32bc-4c35-ac9e-cde0d798f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a850f75-d6cd-4b0c-869d-aebf5ee42713",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_timestamp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3️⃣ 시간 데이터 변환 (Timestamp 변환 및 trip_duration 추가)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m final_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mto_timestamp\u001b[49m(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \\\n\u001b[1;32m      3\u001b[0m                    \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_timestamp(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_timestamp' is not defined"
     ]
    }
   ],
   "source": [
    "# 3️⃣ 시간 데이터 변환 (Timestamp 변환 및 trip_duration 추가)\n",
    "final_df = final_df.withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n",
    "                   .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956f9303-999d-43a2-bf9f-b6277faf44b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "[InternalField(dtype=int64, struct_field=StructField('VendorID', LongType(), True)), InternalField(dtype=datetime64[us], struct_field=StructField('tpep_pickup_datetime', TimestampNTZType(), True)), InternalField(dtype=datetime64[us], struct_field=StructField('tpep_dropoff_datetime', TimestampNTZType(), True)), InternalField(dtype=int64, struct_field=StructField('passenger_count', LongType(), True)), InternalField(dtype=float64, struct_field=StructField('trip_distance', DoubleType(), True)), InternalField(dtype=int64, struct_field=StructField('RatecodeID', LongType(), True)), InternalField(dtype=object, struct_field=StructField('store_and_fwd_flag', StringType(), True)), InternalField(dtype=int64, struct_field=StructField('PULocationID', LongType(), True)), InternalField(dtype=int64, struct_field=StructField('DOLocationID', LongType(), True)), InternalField(dtype=int64, struct_field=StructField('payment_type', LongType(), True)), InternalField(dtype=float64, struct_field=StructField('fare_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('extra', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('mta_tax', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('tip_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('tolls_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('improvement_surcharge', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('total_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('congestion_surcharge', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('Airport_fee', DoubleType(), True))]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mps\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# PySpark DataFrame → Pandas API on Spark DataFrame 변환\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ps_df \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Pandas처럼 평균 계산 가능\u001b[39;00m\n\u001b[1;32m      7\u001b[0m avg_trip_duration \u001b[38;5;241m=\u001b[39m (ps_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m-\u001b[39mps_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:5786\u001b[0m, in \u001b[0;36mDataFrame.pandas_api\u001b[0;34m(self, index_col)\u001b[0m\n\u001b[1;32m   5783\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternalFrame\n\u001b[1;32m   5785\u001b[0m index_spark_columns, index_names \u001b[38;5;241m=\u001b[39m _get_index_map(\u001b[38;5;28mself\u001b[39m, index_col)\n\u001b[0;32m-> 5786\u001b[0m internal \u001b[38;5;241m=\u001b[39m \u001b[43mInternalFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspark_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_spark_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_spark_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   5790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PandasOnSparkDataFrame(internal)\n",
      "File \u001b[0;32m~/spark/python/pyspark/pandas/internal.py:787\u001b[0m, in \u001b[0;36mInternalFrame.__init__\u001b[0;34m(self, spark_frame, index_spark_columns, index_names, index_fields, column_labels, data_spark_columns, data_fields, column_label_names)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    781\u001b[0m             index_field\u001b[38;5;241m.\u001b[39mstruct_field \u001b[38;5;241m==\u001b[39m struct_field\n\u001b[1;32m    782\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m index_field, struct_field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(index_fields, struct_fields)\n\u001b[1;32m    783\u001b[0m         ), (index_fields, struct_fields)\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_fields: List[InternalField] \u001b[38;5;241m=\u001b[39m index_fields\n\u001b[0;32m--> 787\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(ops\u001b[38;5;241m.\u001b[39mdtype, Dtype\u001b[38;5;241m.\u001b[39m__args__)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    790\u001b[0m         ops\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m as_spark_type(ops\u001b[38;5;241m.\u001b[39mdtype, raise_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m     )\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ops \u001b[38;5;129;01min\u001b[39;00m data_fields\n\u001b[1;32m    794\u001b[0m ), data_fields\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_testing():\n\u001b[1;32m    797\u001b[0m     struct_fields \u001b[38;5;241m=\u001b[39m spark_frame\u001b[38;5;241m.\u001b[39mselect(data_spark_columns)\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mfields\n",
      "\u001b[0;31mAssertionError\u001b[0m: [InternalField(dtype=int64, struct_field=StructField('VendorID', LongType(), True)), InternalField(dtype=datetime64[us], struct_field=StructField('tpep_pickup_datetime', TimestampNTZType(), True)), InternalField(dtype=datetime64[us], struct_field=StructField('tpep_dropoff_datetime', TimestampNTZType(), True)), InternalField(dtype=int64, struct_field=StructField('passenger_count', LongType(), True)), InternalField(dtype=float64, struct_field=StructField('trip_distance', DoubleType(), True)), InternalField(dtype=int64, struct_field=StructField('RatecodeID', LongType(), True)), InternalField(dtype=object, struct_field=StructField('store_and_fwd_flag', StringType(), True)), InternalField(dtype=int64, struct_field=StructField('PULocationID', LongType(), True)), InternalField(dtype=int64, struct_field=StructField('DOLocationID', LongType(), True)), InternalField(dtype=int64, struct_field=StructField('payment_type', LongType(), True)), InternalField(dtype=float64, struct_field=StructField('fare_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('extra', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('mta_tax', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('tip_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('tolls_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('improvement_surcharge', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('total_amount', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('congestion_surcharge', DoubleType(), True)), InternalField(dtype=float64, struct_field=StructField('Airport_fee', DoubleType(), True))]"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# PySpark DataFrame → Pandas API on Spark DataFrame 변환\n",
    "ps_df = final_df.pandas_api()\n",
    "\n",
    "# Pandas처럼 평균 계산 가능\n",
    "avg_trip_duration = (ps_df[\"tpep_dropoff_datetime\"]-ps_df[\"tpep_pickup_datetime\"]).mean()\n",
    "avg_trip_distance = ps_df[\"trip_distance\"].mean()\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"📌 평균 이동 시간: {avg_trip_duration:.2f} 초\")\n",
    "print(f\"📌 평균 이동 거리: {avg_trip_distance:.2f} 마일\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782e557-5f85-449b-ad28-56677d00356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe38ffb-b7f2-46e4-91b3-5509e5d85e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 평균 이동 시간: 1796.49 초\n",
      "📌 평균 이동 거리: 5.01 마일\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, mean, unix_timestamp\n",
    "\n",
    "# 이동 시간(trip_duration) 계산 (초 단위)\n",
    "final_df = final_df.withColumn(\"trip_duration\", \n",
    "                               unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# 평균 이동 시간(trip_duration) 계산\n",
    "avg_trip_duration = final_df.select(mean(col(\"trip_duration\"))).collect()[0][0]\n",
    "\n",
    "# 평균 이동 거리(trip_distance) 계산\n",
    "avg_trip_distance = final_df.select(mean(col(\"trip_distance\"))).collect()[0][0]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"📌 평균 이동 시간: {avg_trip_duration:.2f} 초\")\n",
    "print(f\"📌 평균 이동 거리: {avg_trip_distance:.2f} 마일\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19396ac-0693-4fda-9413-a4ba9f84b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType, IntegerType, DoubleType, StringType\n",
    "\n",
    "# 스파크 세션 시작\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ParquetFileReader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 벡터화된 리더 비활성화\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "# 여러 파일을 읽기 전에 모든 컬럼을 String 타입으로 강제 변환\n",
    "df = spark.read.parquet(\"file:///Users/admin/Desktop/GitHub/softeer/과제/M4/NYC_TLC_Trip_Data/*.parquet\")\n",
    "\n",
    "# 모든 컬럼을 String 타입으로 변환\n",
    "for col_name in df.columns:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(StringType()))\n",
    "\n",
    "# 필요한 타입으로 변환\n",
    "df = df.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType()))\n",
    "df = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(IntegerType()))\n",
    "df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(DoubleType()))\n",
    "df = df.withColumn(\"total_amount\", col(\"total_amount\").cast(DoubleType()))\n",
    "\n",
    "# 스키마 출력\n",
    "df.printSchema()\n",
    "\n",
    "# 데이터 확인 (상위 5개 행 출력)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c784c9-8d1f-4def-8ba9-8647da104679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# 1️⃣ Spark 세션 생성\n",
    "spark = SparkSession.builder.appName(\"YellowTaxiMerge\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "# 2️⃣ 모든 Parquet 파일을 읽어서 하나의 DataFrame으로 병합\n",
    "file_path = \"/Users/admin/Desktop/GitHub/softeer/과제/M4/NYC_TLC_Trip_Data/*.parquet\"\n",
    "df = spark.read.option(\"mergeSchema\", \"false\").parquet(file_path)\n",
    "\n",
    "# 3️⃣ 수동으로 컬럼 타입 강제 변환 (LongType으로 변환)\n",
    "df = df.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType()))\n",
    "df = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType()))\n",
    "df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(LongType()))\n",
    "df = df.withColumn(\"payment_type\", col(\"payment_type\").cast(LongType()))\n",
    "df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"extra\", col(\"extra\").cast(LongType()))\n",
    "df = df.withColumn(\"mta_tax\", col(\"mta_tax\").cast(LongType()))\n",
    "df = df.withColumn(\"tip_amount\", col(\"tip_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"improvement_surcharge\", col(\"improvement_surcharge\").cast(LongType()))\n",
    "df = df.withColumn(\"total_amount\", col(\"total_amount\").cast(LongType()))\n",
    "df = df.withColumn(\"congestion_surcharge\", col(\"congestion_surcharge\").cast(LongType()))\n",
    "df = df.withColumn(\"airport_fee\", col(\"airport_fee\").cast(LongType()))\n",
    "\n",
    "# 4️⃣ 스키마 출력\n",
    "df.printSchema()\n",
    "\n",
    "# 5️⃣ 데이터 확인 (상위 5개 행 출력)\n",
    "df.take(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd8c08-aa75-4448-9eb0-07e577c9aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Schema Check\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 파일 목록 가져오기\n",
    "import glob\n",
    "\n",
    "parquet_files = glob.glob(\"/Users/admin/Desktop/GitHub/softeer/과제/M4/NYC_TLC_Trip_Data/*.parquet\")\n",
    "\n",
    "# 각 파일의 스키마 확인\n",
    "for file in parquet_files:\n",
    "    print(f\"\\n📂 파일: {file}\")\n",
    "    df = spark.read.parquet(file)\n",
    "    df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80414da-9f2c-43f9-98d2-ab8fb0be22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# 기존 SparkContext 종료\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "# 새로운 Spark 세션 생성\n",
    "spark = SparkSession.builder.appName(\"YellowTaxiMerge\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e89866-7187-4894-a2c2-8d49828caf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1db9c-4c8b-4e52-ba97-97ab9ea5d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Spark 세션 생성\n",
    "spark = SparkSession.builder.appName(\"Yellow Taxi Data\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\").getOrCreate()\n",
    "\n",
    "# 전체 디렉터리에서 Parquet 파일 읽기\n",
    "df = spark.read.parquet(\"NYC_TLC_Trip_Data/\") \\\n",
    "    .withColumn(\"VendorID\", col(\"VendorID\").cast(\"bigint\"))  # VendorID를 bigint로 변환\n",
    "\n",
    "# 스키마 확인 및 데이터 출력\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe926c5-1b9d-42f1-b261-5bea3180190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44e787-de90-463c-9643-340b713c5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 SparkContext가 있는 경우 종료\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9a5e3-db0b-4637-bb97-6c5c9ae5c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 기존 SparkSession 종료\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    print(\"No existing SparkSession to stop:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d41c3d-f09b-43b5-8cf7-7ca323735d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test Spark Session\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8906bf-cf54-4893-87bf-669d11bc3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 기존 SparkSession 및 SparkContext 종료\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"No active SparkSession or failed to stop:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68590684-ed3f-4e3f-9534-42ea7f2bc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b594222-f425-41e5-9f10-f227d7527e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 기존 SparkSession 종료\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped.\")\n",
    "except Exception as e:\n",
    "    print(\"No active SparkSession or failed to stop:\", e)\n",
    "\n",
    "# 새로운 SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark 종료\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d85a2-7b44-4cfb-9183-4ea5d0ded82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 기존 SparkSession 종료\n",
    "try:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.stop()\n",
    "    print(\"Existing SparkSession stopped.\")\n",
    "except Exception as e:\n",
    "    print(\"No active SparkSession or failed to stop:\", e)\n",
    "\n",
    "# 새로운 SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark 종료\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e6127-e5b8-4a65-8c12-222b42d3d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark 종료\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ecaed-a1a0-4057-9b54-24ca0762f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"4041\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark 종료\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7e888-4c30-4194-b5ab-f3c66be46150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"4041\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.net.preferIPv4Stack=true\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Djava.net.preferIPv4Stack=true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "\n",
    "# Spark 종료\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a516711-4905-4526-b1db-ca699c2cb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully.\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b868d1-c1cf-495f-b68f-ebd63298cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkTest\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163ff44-8704-41f3-a029-bcc63acb4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c957b4b-8a70-46f1-af3b-e9da2c69c179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (System)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
